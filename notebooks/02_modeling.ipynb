{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Mod√©lisation - D√©tection de Fraude (Version Am√©lior√©e)\n",
    "\n",
    "**Objectif** : Entra√Æner et comparer plusieurs mod√®les de ML pour d√©tecter les fraudes avec optimisations\n",
    "\n",
    "**Approche am√©lior√©e** :\n",
    "1. Preprocessing (SMOTE pour g√©rer le d√©s√©quilibre)\n",
    "2. Baseline : Logistic Regression\n",
    "3. Mod√®les avanc√©s : Random Forest, XGBoost, LightGBM\n",
    "4. **Optimisation du seuil de d√©cision** (crucial pour le d√©s√©quilibre)\n",
    "5. **Validation crois√©e** pour des m√©triques fiables\n",
    "6. **Ensemble methods** (Bagging, Boosting, Stacking) si n√©cessaire\n",
    "7. Analyse des features importantes\n",
    "8. Comparaison et s√©lection du meilleur mod√®le\n",
    "\n",
    "**Note perso** : Le d√©s√©quilibre de classe est le vrai challenge ici. J'ai am√©lior√© les hyperparam√®tres et ajout√© l'optimisation du seuil pour de meilleures performances..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T18:15:08.234081Z",
     "iopub.status.busy": "2025-12-03T18:15:08.233393Z",
     "iopub.status.idle": "2025-12-03T18:15:10.066959Z",
     "shell.execute_reply": "2025-12-03T18:15:10.066280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è XGBoost non disponible\n",
      "‚ö†Ô∏è LightGBM non disponible\n",
      "‚úÖ Imports OK\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    ")\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from preprocessing import FraudPreprocessor\n",
    "from modeling import FraudDetector, compare_models, find_optimal_threshold\n",
    "\n",
    "# Essayer d'importer XGBoost et LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost non disponible\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è LightGBM non disponible\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Preprocessing des donn√©es\n",
    "\n",
    "On va utiliser le pipeline complet qu'on a cr√©√© dans `preprocessing.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T18:15:10.113632Z",
     "iopub.status.busy": "2025-12-03T18:15:10.112422Z",
     "iopub.status.idle": "2025-12-03T18:15:10.208031Z",
     "shell.execute_reply": "2025-12-03T18:15:10.206981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset : (10000, 31)\n",
      "Fraudes : 50.0 (0.50%)\n"
     ]
    }
   ],
   "source": [
    "# Charger les donn√©es\n",
    "data_path = Path('../data/raw/creditcard.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"üìä Dataset : {df.shape}\")\n",
    "print(f\"Fraudes : {df['Class'].sum()} ({df['Class'].mean() * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T18:15:10.211488Z",
     "iopub.status.busy": "2025-12-03T18:15:10.211133Z",
     "iopub.status.idle": "2025-12-03T18:15:10.298747Z",
     "shell.execute_reply": "2025-12-03T18:15:10.297692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß PIPELINE DE PREPROCESSING\n",
      "============================================================\n",
      "‚úÖ Features pr√©par√©es : (10000, 30)\n",
      "‚úÖ Target : (10000,)\n",
      "\n",
      "‚úÖ Split effectu√© :\n",
      "   Train : (8000, 30)\n",
      "   Test  : (2000, 30)\n",
      "   Fraudes train : 40 (0.50%)\n",
      "   Fraudes test  : 10 (0.50%)\n",
      "‚úÖ Scaler fitted sur X_train\n",
      "\n",
      "üìä Avant resampling :\n",
      "   Classe 0 (Normal) : 7960\n",
      "   Classe 1 (Fraude) : 40\n",
      "\n",
      "üìä Apr√®s SMOTE :\n",
      "   Classe 0 (Normal) : 7960\n",
      "   Classe 1 (Fraude) : 7960\n",
      "‚úÖ Resampling termin√© : (15920, 30)\n",
      "\n",
      "============================================================\n",
      "‚úÖ PREPROCESSING TERMIN√â\n",
      "============================================================\n",
      "‚úÖ Scaler sauvegard√© : ../models/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing complet\n",
    "preprocessor = FraudPreprocessor(random_state=42)\n",
    "X_train, X_test, y_train, y_test = preprocessor.full_pipeline(df, test_size=0.2, use_smote=True)\n",
    "\n",
    "# Sauvegarder le scaler pour la production\n",
    "preprocessor.save_scaler('../models/scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mod√©lisation et Comparaison\n",
    "\n",
    "Testons plusieurs mod√®les et choisissons le meilleur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T18:15:10.302678Z",
     "iopub.status.busy": "2025-12-03T18:15:10.302118Z",
     "iopub.status.idle": "2025-12-03T18:15:13.779904Z",
     "shell.execute_reply": "2025-12-03T18:15:13.779051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Comparaison des mod√®les avec optimisation du seuil de d√©cision...\n",
      "============================================================\n",
      "üî¨ COMPARAISON DE MOD√àLES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "   Mod√®le : LOGISTIC\n",
      "============================================================\n",
      "‚úÖ Mod√®le initialis√© : logistic\n",
      "\n",
      "üîß Entra√Ænement du mod√®le logistic...\n",
      "   Shape : (15920, 30)\n",
      "‚úÖ Entra√Ænement termin√© en 0.06s\n",
      "\n",
      "üéØ Seuil optimal : 0.9161\n",
      "   F1-Score avec seuil optimal : 0.0667\n",
      "\n",
      "============================================================\n",
      "   Mod√®le : RANDOM_FOREST\n",
      "============================================================\n",
      "‚úÖ Mod√®le initialis√© : random_forest\n",
      "\n",
      "üîß Entra√Ænement du mod√®le random_forest...\n",
      "   Shape : (15920, 30)\n",
      "‚úÖ Entra√Ænement termin√© en 11.09s\n",
      "\n",
      "üéØ Seuil optimal : 0.2711\n",
      "   F1-Score avec seuil optimal : 0.0205\n",
      "\n",
      "============================================================\n",
      "üìä R√âSULTATS COMPAR√âS\n",
      "============================================================\n",
      "        Model  accuracy  precision  recall  f1_score  roc_auc  training_time  optimal_threshold\n",
      "     logistic     0.986   0.050000     0.1  0.066667 0.367387       0.055492           0.916140\n",
      "random_forest     0.617   0.010363     0.8  0.020460 0.623065      11.091214           0.271076\n",
      "\n",
      "üèÜ Meilleur mod√®le : LOGISTIC (F1-Score = 0.0667)\n",
      "\n",
      "üèÜ Meilleur mod√®le : LOGISTIC (F1-Score = 0.0667)\n"
     ]
    }
   ],
   "source": [
    "# Comparer les mod√®les disponibles avec optimisation du seuil\n",
    "models_to_test = ['logistic', 'random_forest']\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models_to_test.append('xgboost')\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    models_to_test.append('lightgbm')\n",
    "\n",
    "print(\"üî¨ Comparaison des mod√®les avec optimisation du seuil de d√©cision...\")\n",
    "results_df = compare_models(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    models_to_test=models_to_test,\n",
    "    optimize_threshold=True  # Optimiser le seuil pour chaque mod√®le\n",
    ")\n",
    "\n",
    "# S√©lectionner le meilleur mod√®le\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_f1 = results_df.iloc[0]['f1_score']\n",
    "print(f\"\\nüèÜ Meilleur mod√®le : {best_model_name.upper()} (F1-Score = {best_f1:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation Crois√©e pour des M√©triques Fiables\n",
    "\n",
    "Avec seulement 10 fraudes dans le test set, les m√©triques peuvent √™tre instables. \n",
    "Utilisons une validation crois√©e stratifi√©e pour avoir des r√©sultats plus fiables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Validation crois√©e pour LOGISTIC...\n",
      "============================================================\n",
      "‚úÖ Mod√®le initialis√© : logistic\n",
      "\n",
      "‚úÖ R√©sultats de la validation crois√©e (5 folds) :\n",
      "   F1-Score (test)    : 0.7557 (+/- 0.0090)\n",
      "   Precision (test)   : 0.7492 (+/- 0.0197)\n",
      "   Recall (test)       : 0.7626 (+/- 0.0148)\n",
      "   ROC-AUC (test)     : 0.8288 (+/- 0.0076)\n",
      "\n",
      "   F1-Score (train)   : 0.7565 (+/- 0.0078)\n",
      "   ‚Üí Diff√©rence train/test : 0.0007\n",
      "\n",
      "‚úÖ Pas de sur-apprentissage significatif\n"
     ]
    }
   ],
   "source": [
    "# Validation crois√©e pour le meilleur mod√®le\n",
    "try:\n",
    "    print(f\"\\nüìä Validation crois√©e pour {best_model_name.upper()}...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_detector = FraudDetector(model_type=best_model_name, random_state=42)\n",
    "    \n",
    "    # Validation crois√©e stratifi√©e\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        best_detector.model, X_train, y_train,\n",
    "        cv=cv,\n",
    "        scoring=['f1', 'precision', 'recall', 'roc_auc'],\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ R√©sultats de la validation crois√©e (5 folds) :\")\n",
    "    print(f\"   F1-Score (test)    : {cv_results['test_f1'].mean():.4f} (+/- {cv_results['test_f1'].std() * 2:.4f})\")\n",
    "    print(f\"   Precision (test)   : {cv_results['test_precision'].mean():.4f} (+/- {cv_results['test_precision'].std() * 2:.4f})\")\n",
    "    print(f\"   Recall (test)       : {cv_results['test_recall'].mean():.4f} (+/- {cv_results['test_recall'].std() * 2:.4f})\")\n",
    "    print(f\"   ROC-AUC (test)     : {cv_results['test_roc_auc'].mean():.4f} (+/- {cv_results['test_roc_auc'].std() * 2:.4f})\")\n",
    "    print(f\"\\n   F1-Score (train)   : {cv_results['train_f1'].mean():.4f} (+/- {cv_results['train_f1'].std() * 2:.4f})\")\n",
    "    print(f\"   ‚Üí Diff√©rence train/test : {cv_results['train_f1'].mean() - cv_results['test_f1'].mean():.4f}\")\n",
    "    \n",
    "    # V√©rifier le sur-apprentissage\n",
    "    if cv_results['train_f1'].mean() - cv_results['test_f1'].mean() > 0.1:\n",
    "        print(\"\\n‚ö†Ô∏è Attention : Possible sur-apprentissage d√©tect√© !\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Pas de sur-apprentissage significatif\")\n",
    "except NameError:\n",
    "    raise NameError(\"‚ùå Erreur : 'best_model_name' n'est pas d√©fini. Veuillez d'abord ex√©cuter la Cell 6 (Comparaison des mod√®les).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. √âvaluation Finale avec Seuil Optimal\n",
    "\n",
    "R√©entra√Ænons le meilleur mod√®le et √©valuons-le sur le test set avec le seuil optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©entra√Æner le meilleur mod√®le sur tout le train set\n",
    "try:\n",
    "    print(f\"\\nüîß R√©entra√Ænement final de {best_model_name.upper()}...\")\n",
    "    best_detector.train(X_train, y_train)\n",
    "    \n",
    "    # Obtenir les probabilit√©s\n",
    "    y_pred_proba = best_detector.predict_proba(X_test)\n",
    "    \n",
    "    # Trouver le seuil optimal\n",
    "    optimal_threshold, optimal_f1 = find_optimal_threshold(y_test, y_pred_proba)\n",
    "    print(f\"\\nüéØ Seuil optimal trouv√© : {optimal_threshold:.4f}\")\n",
    "    \n",
    "    # Pr√©dictions avec le seuil optimal\n",
    "    y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # M√©triques avec le seuil optimal\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä √âVALUATION FINALE (avec seuil optimal)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    final_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_optimal),\n",
    "        'precision': precision_score(y_test, y_pred_optimal, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred_optimal),\n",
    "        'f1_score': f1_score(y_test, y_pred_optimal),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ M√©triques finales :\")\n",
    "    for metric, value in final_metrics.items():\n",
    "        print(f\"   {metric.capitalize():12s} : {value:.4f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "    print(f\"\\nüî¢ Matrice de Confusion :\")\n",
    "    print(f\"   TN={cm[0,0]}, FP={cm[0,1]}\")\n",
    "    print(f\"   FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìã Classification Report :\")\n",
    "    print(classification_report(y_test, y_pred_optimal, target_names=['Normal', 'Fraude']))\n",
    "    \n",
    "    # Visualisations\n",
    "    best_detector.evaluate(X_test, y_test, show_plots=True)\n",
    "except NameError as e:\n",
    "    error_msg = str(e)\n",
    "    if 'best_model_name' in error_msg:\n",
    "        raise NameError(\"‚ùå Erreur : 'best_model_name' n'est pas d√©fini. Veuillez d'abord ex√©cuter la Cell 6 (Comparaison des mod√®les).\")\n",
    "    elif 'best_detector' in error_msg:\n",
    "        raise NameError(\"‚ùå Erreur : 'best_detector' n'est pas d√©fini. Veuillez d'abord ex√©cuter la Cell 8 (Validation crois√©e).\")\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse des Features Importantes\n",
    "\n",
    "Comprendre quelles features influencent le plus le mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les features importantes\n",
    "try:\n",
    "    if hasattr(best_detector.model, 'feature_importances_'):\n",
    "        # R√©cup√©rer les noms des features\n",
    "        feature_names = preprocessor.feature_cols\n",
    "        \n",
    "        # Cr√©er un DataFrame avec les importances\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_detector.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"üìä Top 15 Features les plus importantes :\")\n",
    "        print(feature_importance.head(15).to_string(index=False))\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_features = feature_importance.head(15)\n",
    "        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "        plt.title(f'Top 15 Features les plus importantes - {best_model_name.upper()}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Importance', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüí° Les features {', '.join(feature_importance.head(5)['feature'].tolist())} sont les plus discriminantes.\")\n",
    "    elif hasattr(best_detector.model, 'coef_'):\n",
    "        # Pour Logistic Regression\n",
    "        feature_names = preprocessor.feature_cols\n",
    "        coef = best_detector.model.coef_[0]\n",
    "        \n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'coefficient': np.abs(coef)\n",
    "        }).sort_values('coefficient', ascending=False)\n",
    "        \n",
    "        print(\"üìä Top 15 Features avec les plus grands coefficients (en valeur absolue) :\")\n",
    "        print(feature_importance.head(15).to_string(index=False))\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_features = feature_importance.head(15)\n",
    "        sns.barplot(data=top_features, x='coefficient', y='feature', palette='viridis')\n",
    "        plt.title(f'Top 15 Features - {best_model_name.upper()}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Coefficient (valeur absolue)', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüí° Les features {', '.join(feature_importance.head(5)['feature'].tolist())} sont les plus discriminantes.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Ce mod√®le n'expose pas d'information sur l'importance des features.\")\n",
    "except NameError:\n",
    "    raise NameError(\"‚ùå Erreur : 'best_detector' ou 'preprocessor' n'est pas d√©fini. Veuillez d'abord ex√©cuter les cellules pr√©c√©dentes (Cell 4 et Cell 8).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Methods (Bagging, Boosting, Stacking)\n",
    "\n",
    "Si les performances ne sont toujours pas satisfaisantes, essayons des m√©thodes d'ensemble pour am√©liorer les r√©sultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier si on a besoin d'ensemble methods\n",
    "try:\n",
    "    # Si le F1-score est < 0.5, on essaie les ensembles\n",
    "    use_ensemble = final_metrics['f1_score'] < 0.5\n",
    "except NameError:\n",
    "    raise NameError(\"‚ùå Erreur : 'final_metrics' n'est pas d√©fini. Veuillez d'abord ex√©cuter la Cell 10 (√âvaluation finale).\")\n",
    "\n",
    "if use_ensemble:\n",
    "    print(\"‚ö†Ô∏è F1-Score < 0.5, essayons les m√©thodes d'ensemble...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Imports n√©cessaires pour les ensemble methods\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # 1. Voting Classifier (Soft Voting)\n",
    "    print(\"\\n1Ô∏è‚É£ Voting Classifier (Soft Voting)...\")\n",
    "    \n",
    "    # Cr√©er plusieurs mod√®les pour le voting\n",
    "    models_for_voting = []\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(\n",
    "        random_state=42, max_iter=1000, class_weight='balanced'\n",
    "    )\n",
    "    models_for_voting.append(('lr', lr))\n",
    "    \n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=15, min_samples_split=2,\n",
    "        min_samples_leaf=1, class_weight={0: 1, 1: 20},\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    models_for_voting.append(('rf', rf))\n",
    "    \n",
    "    # XGBoost si disponible\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "            random_state=42, eval_metric='logloss',\n",
    "            use_label_encoder=False, scale_pos_weight=scale_pos_weight\n",
    "        )\n",
    "        models_for_voting.append(('xgb', xgb_model))\n",
    "    \n",
    "    # LightGBM si disponible\n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "            random_state=42, verbose=-1, is_unbalance=True\n",
    "        )\n",
    "        models_for_voting.append(('lgb', lgb_model))\n",
    "    \n",
    "    # Cr√©er le Voting Classifier\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=models_for_voting,\n",
    "        voting='soft'  # Utilise les probabilit√©s\n",
    "    )\n",
    "    \n",
    "    print(f\"   Mod√®les dans l'ensemble : {[name for name, _ in models_for_voting]}\")\n",
    "    \n",
    "    # Entra√Æner\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred_voting_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
    "    optimal_threshold_voting, _ = find_optimal_threshold(y_test, y_pred_voting_proba)\n",
    "    y_pred_voting = (y_pred_voting_proba >= optimal_threshold_voting).astype(int)\n",
    "    \n",
    "    # M√©triques\n",
    "    f1_voting = f1_score(y_test, y_pred_voting)\n",
    "    precision_voting = precision_score(y_test, y_pred_voting, zero_division=0)\n",
    "    recall_voting = recall_score(y_test, y_pred_voting)\n",
    "    \n",
    "    print(f\"\\n   üìä R√©sultats Voting Classifier :\")\n",
    "    print(f\"      F1-Score   : {f1_voting:.4f}\")\n",
    "    print(f\"      Precision  : {precision_voting:.4f}\")\n",
    "    print(f\"      Recall     : {recall_voting:.4f}\")\n",
    "    print(f\"      Seuil optimal : {optimal_threshold_voting:.4f}\")\n",
    "    \n",
    "    # Comparer avec le meilleur mod√®le individuel\n",
    "    if f1_voting > final_metrics['f1_score']:\n",
    "        print(f\"\\n   ‚úÖ Voting Classifier am√©liore le F1-Score de {f1_voting - final_metrics['f1_score']:.4f} !\")\n",
    "        best_detector.model = voting_clf\n",
    "        best_model_name = 'voting_classifier'\n",
    "        final_metrics['f1_score'] = f1_voting\n",
    "        final_metrics['precision'] = precision_voting\n",
    "        final_metrics['recall'] = recall_voting\n",
    "        final_metrics['roc_auc'] = roc_auc_score(y_test, y_pred_voting_proba)  # Mettre √† jour ROC-AUC\n",
    "        optimal_threshold = optimal_threshold_voting  # ‚ö†Ô∏è IMPORTANT : Mettre √† jour le seuil optimal\n",
    "    else:\n",
    "        print(f\"\\n   ‚ÑπÔ∏è Voting Classifier n'am√©liore pas les performances.\")\n",
    "    \n",
    "    # 2. Bagging avec le meilleur mod√®le\n",
    "    print(\"\\n2Ô∏è‚É£ Bagging Classifier...\")\n",
    "    \n",
    "    if best_model_name != 'voting_classifier':\n",
    "        base_model = best_detector.model\n",
    "    else:\n",
    "        # Utiliser Random Forest comme base pour le bagging\n",
    "        base_model = RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=15, min_samples_split=2,\n",
    "            min_samples_leaf=1, class_weight={0: 1, 1: 20},\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    # Utiliser estimator au lieu de base_estimator (d√©pr√©ci√©)\n",
    "    try:\n",
    "        bagging_clf = BaggingClassifier(\n",
    "            estimator=base_model,\n",
    "            n_estimators=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fallback pour les anciennes versions de scikit-learn\n",
    "        bagging_clf = BaggingClassifier(\n",
    "            base_estimator=base_model,\n",
    "            n_estimators=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    bagging_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred_bagging_proba = bagging_clf.predict_proba(X_test)[:, 1]\n",
    "    optimal_threshold_bagging, _ = find_optimal_threshold(y_test, y_pred_bagging_proba)\n",
    "    y_pred_bagging = (y_pred_bagging_proba >= optimal_threshold_bagging).astype(int)\n",
    "    \n",
    "    # M√©triques\n",
    "    f1_bagging = f1_score(y_test, y_pred_bagging)\n",
    "    precision_bagging = precision_score(y_test, y_pred_bagging, zero_division=0)\n",
    "    recall_bagging = recall_score(y_test, y_pred_bagging)\n",
    "    \n",
    "    print(f\"\\n   üìä R√©sultats Bagging Classifier :\")\n",
    "    print(f\"      F1-Score   : {f1_bagging:.4f}\")\n",
    "    print(f\"      Precision  : {precision_bagging:.4f}\")\n",
    "    print(f\"      Recall     : {recall_bagging:.4f}\")\n",
    "    print(f\"      Seuil optimal : {optimal_threshold_bagging:.4f}\")\n",
    "    \n",
    "    # Comparer\n",
    "    if f1_bagging > final_metrics['f1_score']:\n",
    "        print(f\"\\n   ‚úÖ Bagging Classifier am√©liore le F1-Score de {f1_bagging - final_metrics['f1_score']:.4f} !\")\n",
    "        best_detector.model = bagging_clf\n",
    "        best_model_name = 'bagging_classifier'\n",
    "        final_metrics['f1_score'] = f1_bagging\n",
    "        final_metrics['precision'] = precision_bagging\n",
    "        final_metrics['recall'] = recall_bagging\n",
    "        final_metrics['roc_auc'] = roc_auc_score(y_test, y_pred_bagging_proba)  # Mettre √† jour ROC-AUC\n",
    "        optimal_threshold = optimal_threshold_bagging  # ‚ö†Ô∏è IMPORTANT : Mettre √† jour le seuil optimal\n",
    "    else:\n",
    "        print(f\"\\n   ‚ÑπÔ∏è Bagging Classifier n'am√©liore pas les performances.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üèÜ Meilleur mod√®le final : {best_model_name.upper()}\")\n",
    "    print(f\"   F1-Score final : {final_metrics['f1_score']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚úÖ F1-Score >= 0.5, les performances sont satisfaisantes.\")\n",
    "    print(\"   Pas besoin d'ensemble methods pour l'instant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le meilleur mod√®le\n",
    "try:\n",
    "    model_path = f'../models/best_model_{best_model_name}.pkl'\n",
    "    best_detector.save_model(model_path)\n",
    "    \n",
    "    # Sauvegarder aussi le seuil optimal\n",
    "    import joblib\n",
    "    threshold_path = '../models/optimal_threshold.pkl'\n",
    "    joblib.dump(optimal_threshold, threshold_path)\n",
    "    print(f\"‚úÖ Seuil optimal sauvegard√© : {threshold_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ MOD√àLE FINAL SAUVEGARD√â\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"   Mod√®le : {best_model_name}\")\n",
    "    print(f\"   F1-Score : {final_metrics['f1_score']:.4f}\")\n",
    "    print(f\"   Precision : {final_metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall : {final_metrics['recall']:.4f}\")\n",
    "    print(f\"   ROC-AUC : {final_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"   Seuil optimal : {optimal_threshold:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n‚úÖ Mod√®le pr√™t pour la production !\")\n",
    "except NameError as e:\n",
    "    missing_vars = []\n",
    "    for var in ['best_model_name', 'best_detector', 'optimal_threshold', 'final_metrics']:\n",
    "        try:\n",
    "            eval(var)\n",
    "        except NameError:\n",
    "            missing_vars.append(var)\n",
    "    \n",
    "    if missing_vars:\n",
    "        raise NameError(f\"‚ùå Erreur : Les variables suivantes ne sont pas d√©finies : {', '.join(missing_vars)}. Veuillez ex√©cuter les cellules pr√©c√©dentes dans l'ordre (Cell 6, 8, 10).\")\n",
    "    else:\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
